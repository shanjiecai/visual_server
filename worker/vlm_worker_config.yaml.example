# 视觉大模型工作进程配置文件示例
# 复制此文件为 vlm_worker_config.yaml 并根据需要修改配置

# 工作进程基本配置
worker:
  worker_id: "vlm_worker_main"  # 工作进程ID，留空则自动生成
  poll_interval: 1              # 轮询间隔（秒），建议1-2秒
  batch_size: 1                 # 批处理大小，目前建议设为1
  max_retries: 3                # 最大重试次数
  health_check_interval: 30.0   # 健康检查间隔（秒）

# 消息队列配置
queue_config:
  bootstrap_servers: ["localhost:9092"]  # Kafka服务器地址
  topic_name: "demo"                     # 主题名称，需与demo配置一致
  consumer_group: "vlm_workers"          # 消费者组
  use_kafka: true                        # 是否使用Kafka（false则使用内存队列）
  max_request_size: 10485760             # 10MB，最大请求大小
  timeout_default: 30.0                  # 默认超时时间（秒）
  serialize_messages: true               # 是否序列化消息

# 视觉大模型处理器配置
vlm_config:
  # === 阿里云通义千问VL配置示例 ===
  base_url: "https://dashscope.aliyuncs.com/compatible-mode/v1"
  api_key: "YOUR_DASHSCOPE_API_KEY"      # 请替换为你的API密钥
  model_name: "qwen-vl-plus"             # 模型名称: qwen-vl-plus, qwen-vl-max
  
  # === OpenAI GPT-4V配置示例 ===
  # base_url: "https://api.openai.com/v1"
  # api_key: "YOUR_OPENAI_API_KEY"
  # model_name: "gpt-4-vision-preview"
  
  # === 自部署模型配置示例 ===
  # base_url: "http://localhost:8000/v1"
  # api_key: "EMPTY"
  # model_name: "Qwen2.5-VL-72B-Instruct-AWQ"
  
  # 通用参数配置
  max_tokens: 256       # 最大生成令牌数，建议256-512
  temperature: 0.7      # 温度参数（生成多样性），0.1-1.0
  stream: false         # 是否使用流式响应
  timeout: 30.0         # 请求超时时间（秒）
  
  # 默认系统提示词（当消息中没有提供时使用）
  default_system_prompt: |
    你是一个友好的智能助手，能够分析图像内容并与用户进行自然对话。
    
    请根据提供的图像内容，用温暖友好的语气回答用户的问题或完成指定的任务。
    
    请用中文回答，保持回答简洁明了且自然友好。

# 日志配置
logging:
  level: "INFO"                          # 日志级别：DEBUG, INFO, WARNING, ERROR
  file_path: "logs/vlm_worker.log"       # 日志文件路径
  max_file_size: 10485760                # 10MB，日志文件最大大小
  backup_count: 5                        # 保留的备份日志文件数量
  console_output: true                   # 是否输出到控制台

# 配置说明：
# 
# 1. API密钥配置：
#    - 阿里云：在 https://dashscope.console.aliyun.com/ 获取API密钥
#    - OpenAI：在 https://platform.openai.com/api-keys 获取API密钥
#    - 其他服务商：参考各自的文档获取API密钥
#
# 2. 模型选择：
#    - qwen-vl-plus：通义千问VL增强版，性能较好
#    - qwen-vl-max：通义千问VL旗舰版，性能最佳但成本较高
#
# 3. 性能调优：
#    - poll_interval：轮询间隔，过小会增加CPU使用，过大会增加延迟
#    - max_tokens：生成长度，根据需要调整，过大会增加成本和延迟
#    - temperature：创造性，0.1-0.3适合事实性任务，0.7-1.0适合创意任务
#
# 4. 队列配置：
#    - topic_name：必须与demo.py中的配置一致
#    - use_kafka：如果没有Kafka服务，可以设为false使用内存队列
#
# 5. 环境变量覆盖：
#    - VLM_API_KEY：覆盖api_key配置
#    - VLM_BASE_URL：覆盖base_url配置
#    - VLM_MODEL：覆盖model_name配置
#    - KAFKA_HOST：覆盖Kafka主机地址
#    - KAFKA_TOPIC：覆盖主题名称 